{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'subtitle', 'author', 'url', 'text', 'time', 'date',\n",
      "       'difficulty', 'tags'],\n",
      "      dtype='object')\n",
      "                                               title subtitle          author  \\\n",
      "0  Вакуумные флуктуации нарушили механизм квантов...      NaN  Марат Хамадеев   \n",
      "1  Начались наблюдения на самом чувствительном де...      NaN  Тимур Кешелава   \n",
      "2  Атомный ледокол «Арктика» вышел на ходовые исп...      NaN   Василий Сычев   \n",
      "\n",
      "                                                url  \\\n",
      "0  https://nplus1.ru/news/2022/03/03/vacuum-to-hall   \n",
      "1          https://nplus1.ru/news/2019/04/26/LHAASO   \n",
      "2          https://nplus1.ru/news/2019/12/12/arctic   \n",
      "\n",
      "                                                text   time      date  \\\n",
      "0  Европейские физики экспериментально доказали, ...  21:00  03.03.22   \n",
      "1  В Китае стартовала\\nнаучная программа новой ус...  15:16  26.04.19   \n",
      "2  Перспективный российский атомный ледокол «Аркт...  14:36  12.12.19   \n",
      "\n",
      "   difficulty                      tags  \n",
      "0         8.2                ['Физика']  \n",
      "1         2.5  ['Физика', 'Астрономия']  \n",
      "2         3.1             ['Транспорт']  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"news_data_extra.csv\")\n",
    "print(df.columns)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "index_name = \"news_extra_extra3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Индекс news_extra_extra3 успешно создан с русским анализатором\n"
     ]
    }
   ],
   "source": [
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"russian_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": \"_russian_\"  # стандартные русские стоп-слова\n",
    "                },\n",
    "                \"russian_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"language\": \"russian\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"russian_custom\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"russian_stop\",\n",
    "                        \"russian_stemmer\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"text\", \"analyzer\": \"russian_custom\"},\n",
    "            \"subtitle\": {\"type\": \"text\", \"analyzer\": \"russian_custom\"},\n",
    "            \"author\": {\"type\": \"keyword\"},\n",
    "            \"url\": {\"type\": \"keyword\", \"index\": False},\n",
    "            \"text\": {\"type\": \"text\", \"analyzer\": \"russian_custom\"},\n",
    "            \"time\": {\"type\": \"keyword\"},\n",
    "            \"date\": {\"type\": \"date\", \"format\": \"dd.MM.yy||yyyy-MM-dd\"},\n",
    "            \"difficulty\": {\"type\": \"float\"},\n",
    "            \"tags\": {\"type\": \"keyword\"},\n",
    "            \"true_title\": {\"type\": \"text\", \"index\": False},\n",
    "            \"content_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 1024,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es.indices.create(index=index_name, body=mapping)\n",
    "print(f\"Индекс {index_name} успешно создан с русским анализатором\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vadim\\Desktop\\7 сем\\Поиск\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV-файл успешно предобработан и загружен в Elasticsearch\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from normalize_utils import (\n",
    "    normalize_date,\n",
    "    normalize_difficulty,\n",
    "    normalize_tags,\n",
    "    lemmatize_text,\n",
    "    clean_text\n",
    ")\n",
    "from synonyms_utils import get_sentence_embedding\n",
    "\n",
    "with open(\"news_data_extra.csv\", newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        row['true_title'] = row['title']\n",
    "        row[\"date\"] = normalize_date(row.get(\"date\", \"\"))\n",
    "        row[\"difficulty\"] = normalize_difficulty(row.get(\"difficulty\"))\n",
    "        row[\"tags\"] = normalize_tags(row.get(\"tags\"))\n",
    "\n",
    "        # Лемматизируем большие текстовые поля\n",
    "        for field in [\"title\", \"subtitle\", \"text\"]:\n",
    "            if row.get(field):\n",
    "                row[field] = lemmatize_text(clean_text(row[field]))\n",
    "\n",
    "        title = \" \".join(row[\"title\"]) if isinstance(row.get(\"title\"), list) else row.get(\"title\", \"\")\n",
    "        subtitle = \" \".join(row[\"subtitle\"]) if isinstance(row.get(\"subtitle\"), list) else row.get(\"subtitle\", \"\")\n",
    "\n",
    "        content_text = \". \".join(filter(None, [title, subtitle]))\n",
    "\n",
    "        row[\"content_vector\"] = get_sentence_embedding(content_text)[0].tolist()\n",
    "        es.index(index=index_name, document=row)\n",
    "\n",
    "print(\"CSV-файл успешно предобработан и загружен в Elasticsearch\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
